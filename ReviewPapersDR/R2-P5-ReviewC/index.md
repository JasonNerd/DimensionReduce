---
title: "D:\User\mrrai\project\DimensionReduce\ReviewPapersDR\P5-ReviewC"
date: 2024-02-23T20:19:45+08:00
draft: false
tags: []
categories: ["学习笔记"]
twemoji: true
lightgallery: true
---

Various dimension reduction techniques for high dimensional data analysis: a review

-------------------------
Abstart
在医疗保健时代及其相关研究领域，高维数据的维数问题是一个巨大的挑战，因为它包含了大量的变量，形成了复杂的数据矩阵。为了实现数据的预测、分析和可视化，对复杂数据降维的需求越来越大。一般而言，降维技术被定义为将数据集从高维矩阵压缩到低维矩阵。已经实现了几种用于数据降维的计算技术，并进一步将其分为特征提取和特征选择两大类。在这篇综述中，对各种特征提取和特征选择方法进行了详细的研究，系统地比较了几种用于分析高维数据的降维技术，并克服了数据丢失的问题。然后，还列举了一些案例研究，通过考虑技术文献中描述的一些进展来验证数据降维的更好方法。这篇综述论文可以指导研究人员选择最有效的方法对高维数据进行满意的分析。


-------------------------
1 Introduction
在技术突飞猛进的时代，妥善处理数据至关重要。否则就有数据丢失的机会。有时提取的特征维数较高，难以处理。对这样的高维数据进行分类也是有问题的。由于高维数据集会产生一些冗余特征，从而导致数据丢失。这种高维数据( HDD )无法通过常规的数据库管理来处理。本文研究了不同的特征提取和特征选择技术，以适当地减少用于进一步分类的数据。HDD分析的意义正在不断改进。HDD是大数据的子集，它是大量数据的集合，这些数据可以被分析以发现有用的信息和模式。本次调查的主要目的是指导研究者选择最有效的方法对高维数据进行满意的分析。用于分类的数据应该是突出的，否则不会显示出准确的结果或者会进一步导致数据丢失。因此，为了对数据进行适当的处理，并确保数据在实验过程中不会丢失，必须考虑适当的还原技术。本文引用一些案例对健康医疗大数据分析的约简技术进行了简要探讨。

近来，"高维数据" ( High Dimension Data，HDD )一词成为医学、数据科学和医疗保健领域的热词(亚历山大和王2017 ; Hossain和Muhammad 2016)。它的应用对数据分析、可视化、处理和分类产生了巨大的影响。大量的病人数据可以被记录，这些数据可以被机器学习利用，从而使医疗保健部门受益( Archenaa和Mary Anita 2015)。一个数据集表示一个以域或主题为行、以变量为列的统计数据矩阵。来自输入数据集的单个列称为特征。从技术上讲，一个特征是采样数据的可测性质。HDD取决于3个因素：数据速率(数据产生的速率)、数据准确度(数据类型)和数据量。在数据解译、管理、分析和可视化( Raghupati和Raghupati 2014)等方面具有一定的应用价值。但是HDD这种海量特性的存储、处理和维护需要大量的内存空间，可能造成数据丢失。进一步的数据隐私性、全局数据透明性、数据存储和安全性是数据分析( Deyan和Zhao 2012)研究领域不可回避的问题。该问题导致了一个无法避免的问题，即"维度诅咒" ( Van der Linden和迪弗雷纳2017)，并可能会增加易形成稀疏数据的数据量。当对HDD进行数学分析或统计分析时，稀疏性会产生问题。稀疏数据集包含一个大的变量矩阵；然而，矩阵的单元格可能不包含实际数据。这些单元格只用数字" 0 "来填充，因此会占用更多的内存，造成内存的浪费。这可能是数据丢失( Al - Bakri和Soukaena 2018)的关键因素。因此，通过降维技术( DRT )来降低数据的维度或属性是非常必要的。一般而言，DRT在不影响数据显著特征的情况下，将高阶维度映射为二维或三维形式进行分析( Tan 2018 )。首先，对数据的大维度进行管理，将其降至低维空间，保留数据属性的原始性。在下一步的工作中，以精简的形式将数据发送出去进行处理和分析。图1以框图的形式概括了DRT的整体概念

从图1可以看出，DRT的重点是将数据从高维空间转换到低维空间，以便更好地进行数据分析，保留其原始形式的大部分本质属性。为了对讨论进行统计评估，本文进一步将DRT分为FEA和FSA两类。由于我们的目标是对数据进行全面的理解，早期的研究未能讨论一些重要的分类学问题，这些问题在专家和初学者之间造成了信息鸿沟，特别是在生物医学科学及其应用方面只领先一步的人。多年来，研究者们一直在争论，认为特征子集的最优选择可以得到准确的模型；而另一些人则主张寻找包含特征变换(蒋冠宏、李晓萍2015 ; Ozdenizci和Erdogmus 2019 ; Xu等2017)的最佳特征子集。降维是与该问题相关的主要课题，是指将高维数据转换为低维表示。特征提取是将原始数据在时间域或频率域或时间-频率域转换为海量特征子集的过程。然而，特征选择是从特征子集中选择一些最佳特征的先验技术，在数据解释和分析中提高了研究领域。当我们想要确定大多数可接近数据预测( Ding et al 2012 ; Chandrashekar and Sahin 2014 ; Kira and伦德尔1992 ; Liu and Motoda 2007)的"最佳"特征子集时，需要进一步的特征选择方案。

从文献调研中可以观察到，搜索一个期望的特征子集的首要目标是找到一组能够用于分析原始高维数据集并具有有效分类精度的特征。为了实现这一主要目标，必须成功实现的子目标是将复杂的高维数据转换为低维数据矩阵。不同的算法可能会调用不同的目标，这可能会转移我们搜索的主要目标。因此，我们描述不同算法的方法应该是简短的，并且限制在一些常用的方法( Mazomenos等2013 ;阿加沃尔和Cheng 2012 ; McDonnell等2010)之内。

论文的其余部分安排如下。第二节重点介绍了一些特征提取算法。在Sect . 3、对提出的一些特征选择算法进行了描述。第四部分是比较研究，报告了总体比较分析。实验装置已在Sect . 5进行验证分析。最终Sect .⑥一目了然地描述了结论。



-------------------------
2. Feature extraction algorithms (FEA)
在这一部分中，对特征提取算法进行了讨论。降维技术最古老的属性是特征提取( Gedik 2016 )。它是用于特征变换的最重要的降维技术。在早期，各种传统的算法被用于从原始数据集中提取所需的特征进行数据预处理，如图2所示。

在图2中，" PCA "代表主成分分析，" LDA "代表局部判别分析，" CCA "是典型相关分析，" NMF "代表非负矩阵分解。预处理从数据变换开始，包括标准化、归一化、噪声过滤和特征提取4个方面。让我们举例说明该方案的简单性。一个n维的数据矩阵' A '表示为：

其中a1，a2，..，an是数据元素。分量fi为数据向量的特征。数据变换的目的是将原始的特征向量ai变换成nT维的新的特征向量fTi。虽然我们处理的是单一属性，但特征可能会有较大的变化。例如，我们取向量fi = [ f1、f2]中的两个特征，然而这两个向量的测量缩放因子可能不同。因此，比较或添加这些因素都不能得到令人满意的结果。这个问题可以通过缩放特定范围内的特征来解决，如式( 1 )所示。

这里Γ i和μ i分别是原始特征' fi '的标准差和均值。经过缩放后，特征子集可以以标准化的形式进行重构。这可以通过将数据矩阵' A '除以总计数进行归一化来实现，以保持数据库的准确性和效率。然而，使用聚类技术处理大规模数据是低效的。因此，FEA被应用，这是一种降维方法,这将数据简化为特征，使其成为简化的( Mc Donnell et al . 2010年;吉迪克2016)。图2描述了有监督特征提取方法的层次结构。特征提取方法从分量和投影两方面进行分类。进一步通过线性分析如主成分分析( PCA )，局部Fisher判别分析( LFDA )和典型相关分析( CCA )对成分进行分类。通过非负矩阵分解( Non-negative Matrix Factorization，NMF )和基于流形学习的算法(王永进、张军2013 ; Chen和Zhang 2009)等方法对非线性数据进行分析。

2.1 Principal component analysis (PCA)
一种较早也是最流行的数据降维算法是PCA，最早发展于贝赫贝汉et al . ( 2017 )、Kapsoulis et al . ( 2018 )。它是一种正交特征变换技术，将相关的采样变量转换为线性不相关的采样变量。这些新变量与原始变量相似，因此被称为主成分( PC )。这些成分被线性组合以形成各种特征，并有助于计算最大方差( Kapsoulis et al 2018)。通过使用( Kapsoulis et al 2018)的原始特征，结果是可预测的并且容易理解。图3显示了从一组变量中分离出两个主成分，并显示了以椭圆子空间为符号的数据集的PCA。这里，轴1和轴2分别代表第一和第二主成分。提取的特征子集被子空间所包围，保留了外部的其他特征。如Eq . ( 1 )，对" fi "特征进行观察。因此，PC可以计算为：

2.2 Local fisher’s discriminate analysis (LFDA)
Fisher判别分析( Fisher ' s Discriminate Analysis，FDA )是一种传统的数据矩阵投影技术，具有最大区分度，可以有效地提高分类性能和降低维数。该算法的目标是在保留( Zhao和Gao 2015)类最大判别信息的前提下降低数据维度。然而，现有的方法对于多模态的类分离效果并不理想。为了降低HDD的维度，需要保持数据的局部结构。因此，将FDA的概念与局部保持投影方法进行融合，形成了LFDA。它分离了多模态的类结构，有助于同时保持数据的局部结构(丛志刚和段成荣2016 ; Li等. 2015)。LFDA算法如图5所示。

2.3 Canonical correlation analysis (CCA)
多维数据集中两个变量之间的线性关系是通过一种基于相关性的技术来衡量的，称为CCA。对于每个变量，通过使用多元分析( Jendoubi和Strimmer 2019)，找到了关于相关性最优的两个基。我们考虑两个不同的多元数据集，如

其中i = 1，2，..，n是每个数据集中特征变量的实现。μ A和μ B是' A '和' B '的均值向量，表示为：

对于数据集' A '和' B '，中心点可以分别写为' A = A - μ A和' B = B - μ B。然后对数据集的原始特征(变量)进行线性变换，得到WA和WB等新的特征子集。CCA的主要目标是最大化目标函数，即最大化' A '和' B '的原始特征集与变换后的特征集( WA ) T A和( WB ) T B之间的相关性。目标函数在数学上表示为：

其中，简写词' cov '和' var '分别表示' A '和' B '特征集之间的协方差和方差。术语" cov "可以表示为CAB = " οBT "，" var "可以表示为类似于" cov "的CAA = " οAT "和CBB = " B " B " T。记⟨. · ·为统计期望。因此，多元数据集的CCA被写为，

这里取( WA ) T CAAWA = 1和( WB ) T CBBWB = 1两个约束，因此拉格朗日函数可以确定为：

为了得到特征值，必须满足δ Lf δWA = 0和δ Lf δ WB = 0两个条件。将拉格朗日函数代入上述条件，可得( 7 )式可解为：

式中：λ = 2λA = 2 λ B为特征值(丛志刚和段成荣2016 ; Li et al . 2015 ; Jendoubi和Strimmer 2019)。因此，CCA涉及两个目的：一是数据缩减，即使用少量的线性组合来解释两组变量之间的共变。另一个是数据解释，用于寻找解释变量(米夏埃利et al 2016 ; Luo et al 2015 ; Wilms and Croux 2015)集合之间共变的特征。CCA的算法如图6所示。

图6，表明预测器已经从合成预测器通过，然后该结果通过合成标准和其他不同的标准。然后对结果进行了计算。该算法是为了最大化简单相关性而设计的。

2.4 Non‑negative matrix factorization (NMF)
一种新颖的分解含有两个非负因子的数据集的方法被称为基于NMF的特征提取方法。由于两个原因，该方法被限制为非负.第一个原因是基于视觉感知神经元具有非负的放电频率( Varghese等2018)的神经生理学。第二个原因是基于图像处理领域，其中强度图像占据非负值。令我们考虑一个矩阵A，将每个元素aij分解为X和Y的一个基础系数和分解系数。由于X的大小为i × k，Y的大小为k × j；

该数据矩阵的非负矩阵分解可以通过Cost函数来确定分解的层次。在这里，我们选取了两个具有广义KL散度(米夏埃利et al 2016 ; Luo et al 2015)的特征来寻找' A '和' XY '之间的距离。基于KL的代价函数的表达式为：

式( 16 )保证了没有异常值将主导NMF算法的目标函数。因此，标准化的NMF算法可以作为一种新颖的特征提取方案应用于模式识别、文本分类、肿瘤分类等领域。但其存在优化问题、子空间选择、非线性非负特征等缺点。( Wang et al 2018a , b ; Li和Ngom 2013 ; Zeynep等2011)。NMF的算法如图7所示。图7，显示在第一步中相关症状已被去除。第二步，基于NMF的冗余症状识别。第三步将冗余症状按组别进行了转化。在最后一步中选择了特征子集。

3 Feature selection algorithm (FSA)
在这一部分中，对特征选择的算法进行了讨论。任何FSA的目标都是在不进行任何数据变换的情况下，通过从原始数据集中删除不相关的特征来选择最佳的特征集，以用于高维数据的分析。许多研究人员在(贝罗尼卡et al 2017 ; Ang et al 2016)的几篇文章中阐述了他们在确定算法变量选择的最佳目标函数方面的观点。
在各种搜索方法的基础上，进一步将FSAs分支为过滤器、包装器和嵌入方法。通过过滤方法从数据特征中选择最具判别性的特征。通常，过滤方法在分类前进行特征选择，通常涉及两步过程。在第一步中，所有的特征按照一定的标准进行排序。在下一步中，选择排名最高的特征。许多过滤器类型的方法已经被研究，其中大多数方法是单变量的，根据它们的排序优先级来考虑独立或依赖的特征变量。Wrapper方法使用预定义的学习算法选择一个特征子集作为搜索问题，在其中准备各种问题，并与其他组合进行评估和比较。嵌入方法在模型构建过程中进行特征选择( Das 2001 )。表1简要描述了FSA的属性。
在表1中，研究了过滤器类型的方法，其中大多数方法是单变量的，根据它们的排序优先级来考虑独立或依赖的特征变量。FSA的层次结构如图12所示。
从图12中，我们观察到为了区分不同的FSA，考虑了不同的特征排序方法。特征排序得到的结果高度依赖于个体特征。特征子集选择的结果同样适用于目标类。此外，嵌入方法的结果是可由特征子集配置的预后模型。
滤波方法包括ReliefF、MRMR (最小冗余最大相关)等算法。包装器比过滤器速度快得多，它们使用模型假设，将训练数据集中。特征依赖可以通过包装器方法发现。遗传算法( Genetic Algorithm，GA )和粒子群算法( Particle Swarm Optimization，PSO )的混合算法是包裹式FSA最常用的算法之一。
有效的特征排序算法缓解了大规模数据中的维数灾难问题。特征排序又称为变量排序，是通过度量特征相关度( Hsu et al 2011)的打分函数对特征进行排序的过程。得分函数由训练数据集计算得到。它产生了更好的分类性能，并以较低的计算成本形成了一个预测广义模型，该模型根据" n "个显著特征在模型中的优先级进行选择。其结果并非一般意义上的最优，但就计算目的而言，其计算效率较高。涉及GA、PSO、ReliefF、最小冗余最大相关( MRMR )、递归特征消除( RFE )和同时扰动Stochas等算法

Hybridised genetic algorithm and particle swarm optimization (HGAPSO)
各种示意性FSA需要更多的训练样本才能准确地评估数据集。除此之外，耗尽的CPU处理时间和样本的计算时间是寻找"最佳"最优特征的一个长期问题。为了克服这一障碍，一种更快的FSA被引入，称为HGAPSO。通过有效的适应度函数，该方法具有以最少的特征数处理HDD的能力。这里，回顾GA和PSO这两种最流行的优化算法的概念，记为( Al - Rawi和Karajeh 2007)。

受达尔文进化论的启发，一种新的计算方法被称为GA。它是一种随机的特征选择技术，可以获得广泛的可行解。该算法引入一个参数种群，每个参数被选择一个最小的适应度函数，并形成一个唯一的生成来评估选择( Pedram和贝内迪克松2015)的概率。GA的算法如图13所示。GA算法流程如图13所示。GA特征选择包括三个主要步骤来完成。复制、交叉和变异。在该算法中，线性排序方法用于从一组染色体中识别合适的基因。在排序方法中，根据适应度函数对矩阵中所有染色体赋予一定的排序。进一步基于最高秩，对染色体进行选择。适应度函数可以被认为是定义结果精度的方法。Pedram和贝内迪克松( 2015 )报告了基于GA的特征选择的详细过程。

PSO最早是由Kennedy和埃伯哈特作为生物启发技术衍生而来，观察鸟类( Mahale和Chavan 2012)的群集声。与GA不同，种群由一组粒子组成，其中每个粒子是参数的组合。" Swarm "一词由多维空间中的一组粒子组成，每个群体以独特的速度行驶。PSO算法如图14所示。图14展示了PSO的算法，其中每个粒子的当前和前面的位置可以通过存储芯片跟踪，这被分类为"个人最佳位置"和"全局最佳位置"。观察到前一个粒子和它的邻居粒子的非因果行为改变了当前粒子在搜索空间中的速度。一个粒子高度依赖于3个因素：( a )粒子的当前位置，( b )观察粒子的前一位置所需的内存，( c )群体对粒子集的知识。因此，粒子倾向于移动到搜索空间，在那里他们可以以期望的速度( Mahale和Chavan 2012)移动。

通过将基于GA的选择、交叉和变异等步骤与PSO ( Hong and Dong 2004 )中更新的粒子位置和速度相结合，可以将GA与PSO结合起来。因此，HGAPSO是一种集成技术，用于从一组特征数量必须与粒子总维度相等的种群中最优地选择特征。。。。。。。在图15中，绘制了一组随机种群，并通过应用优化算法获得了种群中每个参数的适应度值。然后通过排序的方法，对特征进行降序排列。然后，执行GA和PSO的组合，其中个体种群作为GA的染色体和作为PSO的粒子执行。然后将整个集合划分为两个不同的子集，每个子集保留自己的属性。

3.2 ReliefF
对ReliefF算法采用了滤波方法。尽管数据不完整，但ReliefF根据质量属性估计来选择特征。ReliefF算法是一种流行的特征权重计算方法。该方法可以应用于名义的、数值的、不完整的和有噪声的数据。一般来说，过滤法比包裹法速度快。它是一个独特的过滤式特征选择算法家族，在客观和计算精度之间保持令人满意的平衡。特征相关性通过求解一个优化问题将特征权重定义为一个测量向量。然而，该算法存在频率采样不确定、特征权重实例存在波动等缺点。因此，需要对ReliefF算法进行一定的改进，以解决基于均值方差模型的精度相关问题。通过考虑判别样本数据的均值和方差来估计特征权重，以获得更准确的结果( Zhi et al 2016)。

寻找最优特征的输入是C类和其他属性值的特征权重向量Fw ( A )。输出的目的是确定输入原始信号的属性质量。为了实现这一输出，以下是一些步骤，如图16所示。图16描绘了ReliefF的算法。在步骤1中，样本数据矩阵的权重Fw固定为0。然后，当i = 1，..，n时，过程开始.对于一个实例ri是随机选择的，类( ri )是为每个类C计算的。在下一步中，为每个类C确定k个最近点中的hitshj点和misses点nj ( c )。再次，计算权重向量为，

然后，整个过程重复n次迭代，并将每个权重向量除以n得到一个相关向量。为了验证特征权重向量，取一个阈值Th，并与相关向量进行比较，进行特征的最优选择。
表2描述了ReliefF算法的现状，该算法通过计算特征权重向量比其他包装器方法更有效，并且解决了凸优化问题。

3.3 Minimum redundancy maximum relevance (MRMR)
在期望任何可行的解决方案之前，应该考虑特征相关性的强度。强特征相关性是指在预测精度的基础上去除不相关的特征，而弱特征相关性是指特征对预测精度的贡献。伴随着特征相关性，特征冗余在FSA中也有一定程度的交叉。特征冗余度可以很好地定义为两个完全相关的特征和两个独立的特征(纳吉等2019 ;周黎安等2019 ;王永进等2018a , b ;金星等2017)。在更常见的方式中，特征相关性和特征冗余都依赖于一个特征子集。为了改变这一问题，需要在不影响原始特征集维度的前提下，识别冗余特征或去除非冗余特征。
互信息是从特征空间中寻找和选择相关特征的最有力工具。通常，这些子集中的某些特征包含了特征的某些特征，这些特征可能是冗余的。由于我们的目标是以有限的尺寸因子选择最相关的特征子集，因此开发了一种新的方法，如MRMR选择算法( Zhou et al . 2019年。Wang et al . 2018a , b ;金星等. 2017年)来消除这个问题。
我们考虑两个随机变量A和B使得它们的互信息IM可以定义为互依性的度量，如

式中：p ( a )为A的边际概率，p( a , b)为A和B的联合概率。最优特征的选择包括2个步骤：( a )通过互信息度量待选特征与目标数据集之间的依赖关系( b )最大化依赖关系。因为，依存关系的计算似乎是一项艰巨的工作；因此，我们选择按照公式，通过MRMR准则来寻找依存关系

由图18可知，数据集已被划分为训练集和验证集。然后将训练集的值送入MRMR并选择特征子集。特征子集和验证集通过了质量评估，所选特征通过了停止准则。经过检验，选择最终的特征子集进行下一步处理。

3.4 Recursive feature elimination (RFE)
RFE是一种技术，它包含了特征向量的权重作为最优特征子集预测的一个重要属性。在基于排序优先级的每次迭代后，一定比例的特征被删除。必须注意的是，即使某些特征在目标数据中不能很好地预测，这些特征也能预测最好的结果。当处理含噪数据时，RFE - SVM算法可以提取到足够的信息给各自的权重值。迭代过程持续进行，直到预测误差达到最小值或消除指定的特征值。

在Gysels等人( 2005 )中，RFE结合支持向量机( SVM )被讨论用于分析一个大维数据集，并发现与其他方法相比，结果是准确的。支持向量机( SVM )是由Vapnik在1963年设计的，目的是为了减少最优选择特征的预测误差。SVM在提高FSA的准确性和消除递归技术中的非冗余特征方面起着至关重要的作用。基于RFE的SVM也可以表示为一个后向选择算法来解决基本的优化问题。采用线性核函数的软间隔SVM提高了处理海量特征时的性能质量。具体算法见Sun et al . ( 2013 )。RFE算法如图19所示。

在图19中，RFE的算法被一步步地解释，1 .对所有特征进行训练建模。2 .重复，直到调优集精度下降：·在最后一个模型中按系数大小排序特征·删除排名最低的特征，例如底部10 % ·重新训练模型

RFE是一种用于线性SVM的后向选择封装方法，但也适用于其他线性方法。它需要对线性方法进行非平凡的修改，因为这些方法不会产生特征上的系数。

3.5 Simultaneous perturbation stochastic approximation (SPSA)
通常观察到，由于目标函数没有得到适当的优化，梯度效应无法计算选定的特征。为了避免这一问题，采用一种梯度近似方法，称为SPSA。该方法对随机梯度逼近(泽仁等2018)只需要两个目标函数，因此被证明是有效的。


SPSA的详细算法可以从图20中研究，图20描述了随机选择一个常数有限系数α来获得两个有限常数α 0和α 1。在模拟了这两个常数之后，还计算了令人满意的判据。然后对最优性条件进行了检验。在不可行解的情况下，对每次迭代(泽仁等2018)进行梯度搜索和α值更新。

4 Comparison analysis
基于各种DRT的质量优点和一些缺点的关键因素( Fu 2016 )，比较分析有希望概括出各种DRT之间有价值的理解。为了一目了然，在表3中提到了所有DRT的优缺点，可以注意到，将原始特征转化为新的特征子集可能会造成数据丢失和数据复杂性问题。为了避免这种情况，FSAs被实现，这使得训练模型不再复杂，便于进一步处理。因此，FSA的研究现状不仅有助于解决过拟合问题，而且为克服"维度诅咒" (希拉和吉利斯2015 ; 2017年Jothi和Rajam)提供了新的思路。FEA方法如PCA、LFDA、CCA、NMF和基于流形的算法列于表4。因此，FSA的最新发展对HGAPSO、ReliefF、MRMR、REF和SPSA等几种FSAs是有帮助的，如表4所示。

对医疗保健数据进行了比较分析，并使用不同的降维技术来降低维度，以突出分类。这里讨论的特征提取的不同方法是PCA，LFDA，CCA，NMF和基于流形的算法。对医疗保健数据进行了比较分析，并使用不同的降维技术来降低维度，以突出分类。这里讨论的特征提取的不同方法是PCA，LFDA，CCA，NMF和基于流形的算法。为了明确其意义和优缺点，不同的特征选择方法被分离为过滤器、包装器和嵌入式技术，如HGAPSO、ReliefF、RFE - SVM、MRMR和SPSA等。然而，特征提取在保持信号的原始性方面与特征选择不同，并促进原始特征子集转化为具有不确定数据丢失的新特征子集。但是特征选择方案在不影响原始数据集属性的前提下，对特征进行最优选择。此外，在提取过程中，一些特征可能会转化为新的特征，这可能会导致过拟合问题和数据丢失的悲惨机会。为了克服这些问题，采用了基于FSA的方案，不仅解决了维数灾难问题，而且选择了最冗余的特征，具有准确的可行性。

5 Validation for high dimensional data analysis
在信号处理的研究领域中，HDD analysis 20多年来一直占据主导地位。为了评估高维数据维度的复杂性，我们考虑了两种情况下从监测设备和遥测中测量的医疗数据集。
5.1 Description of data set
微阵列数据库是遗传数据的一大来源，对其进行适当的分析可以加深我们对生物学和医学的理解。许多微阵列实验已经被设计来研究癌症的遗传机制，并应用分析手段来分类不同类型的癌症或区分癌组织和非癌组织。表5和表6中提到的肿瘤医疗数据集具有较高的维度。在这个数据集中，特征数量非常多，为了进行适当的分类，必须减少特征数量。

案例1  
所提出的特征约简算法的有效性可以通过考虑一些实时数据集进行验证。因此，考虑了三个开源数据集，包括三个微阵列数据集(中枢神经系统肿瘤、白血病和结肠肿瘤)。表5总结了用于降维算法的各自特征子集的数据集。表5中，每个障碍的数据集实例从60个到93个不等，特征数从2000个到6000个不等，类别从2个到9个不等。关于该数据集的其他信息可在Alfaar et al ( 2016 )，Stanojevic and Krivokapic ( 2014 )中获得。这里将一些属性不突出的特征作为名义值，并在[ 0、1 ]范围内进行归一化处理。大部分的索引特征已经被去除。然后，利用部分FEA和FSA将原始特征集转换为提取的特征子集，将复杂的高维数据降维成低维矩阵。这里分析的FEA是PCA，LFDA，CCA，NMF和基于流形的技术。对所提出的HGAPSO、ReliefF、MRMR、RFE和SPSA等5种FSA都重复了相同的步骤。
对包括微阵列数据在内的三种不同的高维数据集进行了分析，以减少它们的维数。为此，需要确定它们的必要特征属性，并将其缩小到有限的大小。从FEA和FSA中获得的特征数量分别在表7和表8中描述。
从表7可以观察到，几乎所有的技术都证明了它们在从原始数据集提取特征方面的效率。但是，在NMF算法中，提取的特征数量较少。因此，数据处理变得更容易，提取的特征数量更少，同时降低了数据复杂度。NMF的效率背后的原因是大数据矩阵容易检查而没有任何数据丢失。由表8可知，MRMR方法通过筛选出最佳特征，降低了各特征的冗余度。MRMR方法是最快速的算法，可以减少最多的冗余属性，并简化特征子集矩阵，以执行高效的分类性能。
然而，对比图. 21和22可以发现，随着所选特征数量的减少，特征选择方法可以有效地去除无关特征。



为了执行各种特征提取算法，将癌症数据集分离为训练和测试数据子集。训练和测试数据不合并，以保持每个特征的个体特征。对于下文所述的两种情况，训练模式( Patro和Sahu 2015)的70 %和测试模式的30 %构成测试模型。

6 Conclusion
本文简要介绍了高维数据降维的各种方法。
在线性数据分析的基础上，对特征提取方法进行了分类讨论，分为PCA、LFDA、CCA算法。还考虑了用NMF和基于流形的算法来描述非线性数据的投影方案。为了明确其意义和优缺点，不同的特征选择方法被分为过滤器、包装器和嵌入式技术，如HGAPSO、ReliefF、RFE - SVM、MRMR和SPSA等。然而，特征提取在保持信号的原始性方面与特征选择不同，并促进原始特征子集转化为具有不确定数据丢失的新特征子集。但是，特征选择是在不影响原始数据集属性的前提下，对特征进行最优选择。通过比较不同的FEA，发现基于NMF的算法的准确率在90 %到95 %的范围内，这是其他特征提取技术中最高的。但是，在提取过程中，一些特征可能会转化为新的特征，这可能会导致过拟合问题和数据丢失的悲惨机会。为了克服这些问题，人们提出了引入各种FSA，不仅解决了维数灾难问题，而且选择了最冗余的特征，具有准确的可行性。验证结果表明，MRMR方法的准确率在92 % ~ 97 %之间，相对优于其他所有提出的特征降维技术。通过对大量文献的研究，得出特征选择方法的结论通过去除数据的噪声和降低数据的维度来改进数据的预处理，从而选择相关的特征子集以获得更好的分类性能。








